---
layout: post
title: "Streams"
author: NodeKC
tags:
---

# Streams

Streams are one of the most amazing features of Node. However, it\'s not a new thing. \*nix systems use I/O streams to move data from one process to another.

## Read Streams

The first stream we will discuss is a read stream. As you might expect this stream supports only reading. Let\'s take a look at an example.

Start by creating a file called ```example.txt``` and add whatever contents you like. Create another file called `readable.js` and add the following:


{% highlight javascript %}
var fs = require('fs');

var rs = fs.createReadStream('./example.txt');

rs.on('data', function (data) {
   console.log(data.toString());
});

rs.on('end', function () {
   console.log('end');
});
{% endhighlight %}

Now that we have used a readable stream let\'s write our own.

{% highlight javascript %}
var Readable = require('stream').Readable;
var util = require('util');
util.inherits(TeamStream, Readable);

function TeamStream () {
    Readable.call(this);
    var best_sports_programs = ['K-State Football', 'K-State Basketball', 'K-State Baseball'];
    var i = 0;
    this._read = function () {
        var program = best_sports_programs[i++];
        this.push(program || null);
    }
};

var rs = new TeamStream();
var received = 0;

rs.on('data', function (data) {
    received++;

    console.log(data.toString());

    if (received == 1) {
        console.log('pausing for some time');
        rs.pause();
        setTimeout(function () { rs.resume(); }, 3000);
    }
});

rs.on('end', function () {
    console.log('ended');
});
{% endhighlight %}


We get to learn a few different things in this example. First, we create a new readable stream. The `MyStream` function is inheriting the prototype from Node core's `ReadableStream` and in its constructor calls the `ReadableStream` constructor. We must call the constructor of the core implementation in order to perform the necessary initialization of our stream. We are required to implement the internal method `_read` which is called on every read request. In this example the first read request is issued whenever we subscribe to the `'data'` event. There are other ways to do this which we will cover next.

You will notice in this example that we are able to control the flow of data from a readable stream through `pause` and `resume`. This is incredibly important to allow your application to keep up with incoming data and only receive data when needed.

The above example reads data by the buffer. However, we have more fine grained control over how much data we receive through the `read` method. The next example will give you an idea of the series of events that occurs when reading from a stream that can provide data at various times. This is exactly the situation when dealing with network connections. Feel free to copy and paste this snippet. Study the output and understand the chain of events before moving on.

{% highlight javascript %}
var Readable = require('stream').Readable;
var util = require('util');

var log =  (function () {
    var startTime = new Date().getTime();

    return function (msg) {
        var now = new Date().getTime();
        console.log('+' + Math.floor((now - startTime) / 1000) + ' ' + msg);
    };
})();

util.inherits(LetterStream, Readable);

function LetterStream () {
    Readable.call(this);
    var letters = ['aaaaaa', 'bbbbbb', 'cccccc', 'dddddd'];

    var i = 0;
    this._read = function () {
        log('Internal read called.');
        var letter = letters[i++];
        if (i > 1) {
            var that = this;
            setTimeout(function () {
                log('Pushing data async.');
                that.push(letter);
            }, 2000);
        }
        else
        {
            log('Pushing data.');
            this.push(letter || null);
        }
    };
};

var rs = new LetterStream();

rs.on('end', function () {
    log('End');
});

rs.on('readable', function () {
    log('Stream is readable');

    var c;
    while (c = rs.read(2))
    {
        log(c.toString());
    }

    log('Reached end of readable data available');
});

log('Reached end of program. It\'s now alive because of async callbacks pending.');
{% endhighlight %}

We\'ve successfully read 2 bytes from our underlying stream with each iteration of the loop. Despite the number of calls to `read` we only invoke our internal function when new data is actually needed.

## Write Stream

An example of where write streams are used are HTTP responses or standard output. Just like `ReadableStream`s, `WritableStream`s have a built in mechanism for rate limiting. These streams are aware of when the underlying buffer has been flushed and provide the caller with this information. This is very useful so that you don\'t seen more than a client can handle. If the client isn\'t able to accept information fast enough it will buffer in memory. Let\'s have a look at a simple example of a commonly used write stream. Open a node REPL and input the following:

{% highlight javascript %}
process.stdout.write('Hello World');
{% endhighlight %}

Notice the REPL printed out Hello World and the result of the method call `true`. The return value indicates whether or not the buffer was able to be flushed immediately. If the return value is false that means the data had to be buffered in memory and will be flushed later. True means that the data was immediately written and is not being buffered.

{% highlight javascript %}
var WritableStream = require('stream').Writable;

// By setting the high water mark to 0 we say that the underlying buffer must be flushed immediately.
var ws = new WritableStream({ highWaterMark: 0});

ws._write = function (data, enc, cb) {
   // Unless otherwise specified in options, the string we write further down
   // this file is converted to a buffer.
   console.log('Received', data);

   setTimeout(function () {
      process.stdout.write(data, enc, cb);
   }, 1000);
};

var result = ws.write('This should be buffered\n');

console.log('write return value', result);

//We can get notified when the underlying buffer has been flushed by using a callback

result = ws.write('This will also be buffered\n', function () {
   console.log('We were notified about this buffer being flushed.');
});

console.log('write (2) return value', result);
{% endhighlight %}

## Pipe streams together

Much praise is given to node by its community for providing an extremely simple interface for chaining streams together using `pipe`. All \*nix developers are familar with having the ability to pipe the standard output of one process into the standard input of another using a `|`. Let\'s see how we could take our standard input and direct it to our processes output stream.

{% highlight javascript %}
process.stdin.pipe(process.stdout);
{% endhighlight %}

We can test this by placing the above contents in a file called pipe.js and running the following shell command.

{% highlight bash %}
echo 'This is going to be piped out' | node pipe.js
{% endhighlight %}

How easy was that! If we didn\'t have `pipe` here's how we\'d accomplish the same task. You can see pipe is taking care of a lot for us. What you do not see in this example is that `pipe` is automatically handling backpressure.

{% highlight javascript %}
process.stdin.on('data', function (data) {
   process.stdout.write(data);
});
{% endhighlight %}
